{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2a2d5e",
   "metadata": {},
   "source": [
    "# Collective Communications\n",
    "\n",
    "Collective Communications A **collective communication** is a\n",
    "communication that involves a group (or groups) of processes.\n",
    "\n",
    "-   the group of processes is represented as always as a\n",
    "    *communicator* that provides a context for the\n",
    "    operation,\n",
    "\n",
    "-   Syntax and semantics of the collective operations are consistent\n",
    "    with the syntax and semantics of the point-to-point operations,\n",
    "\n",
    "-   For collective operations, the amount of data sent **must exactly\n",
    "    match** the amount of data specified by the receiver.\n",
    "\n",
    "Mixing type of calls Collective communication calls may use the\n",
    "same communicators as point-to-point communication; Any (conforming)\n",
    "implementation of MPI messages guarantees that calls generated on behalf\n",
    "of collective communication calls will not be confused with messages\n",
    "generated by point-to-point communication.\n",
    "\n",
    "## Broadcast, Gather and Scatter\n",
    "\n",
    "-   The **broadcast** operation. In the broadcast, initially just the first process contains the data $a_0$, but after the broadcast all processes contain it.\n",
    "    \n",
    "    ![broadcast](./figures/broadcast_pattern.png)\n",
    "    \n",
    "-   This is an example of a **one-to-all** communication, i.e., only one\n",
    "    process contributes to the result, while all processes receive the\n",
    "    result.\n",
    "    \n",
    "``` c\n",
    "int MPI_Bcast(void* buffer, int count, \n",
    " MPI_Datatype datatype, int root, MPI_Comm comm)\n",
    "```\n",
    "Broadcasts a message from the process with rank `root` to all\n",
    "processes of the group, itself included.\n",
    "\n",
    "- `void* buffer`   on return, the content of root's buffer is copied to all other\n",
    "    processes.\n",
    "\n",
    "- `int count`   size of the message\n",
    "\n",
    "- `MPI_Datatype datatype`   type of the `buffer`\n",
    "\n",
    "- `int root`   `rank` of the process broadcasting the message\n",
    "\n",
    "- `MPI_Comm comm`   communicator grouping the processes involved in the broadcast\n",
    "    operation\n",
    "\n",
    "The **scatter** and **gather** operations\n",
    "\n",
    "-   In the **scatter**, initially just the first process contains the\n",
    "    data $a_0,\\ldots,a_3$, but after the **scatter** the $j$th process\n",
    "    contains the $a_j$ data.\n",
    "    \n",
    "    ![scatter](./figures/scatter_pattern.png)\n",
    "    \n",
    "\n",
    "-   In the **gather**, initially the $j$th process contains the $a_j$\n",
    "    data, but after the **gather** the first process contains the data\n",
    "    $a_0,\\ldots,a_3$\n",
    "    \n",
    "    ![gather](./figures/gather_pattern.png)\n",
    "    \n",
    "    Each process (root process included) sends the contents of its send \n",
    "    buffer to the root process. The latter receives the messages and stores \n",
    "    them in rank order.\n",
    "\n",
    "    ``` c\n",
    "    int MPI_Gather(const void* sendbuf, int sendcount, \n",
    "      MPI_Datatype sendtype,  void* recvbuf, int recvcount, \n",
    "      MPI_Datatype recvtype, int root, MPI_Comm comm)\n",
    "    ```\n",
    "\n",
    "    - `const void* sendbuf`   starting address of send buffer\n",
    "\n",
    "    - `int sendcount`   number of elements in send buffer\n",
    "\n",
    "    - `MPI_Datatype sendtype`   data type of send buffer elements\n",
    "\n",
    "    - `void* recvbuf`   [address of receive buffer]{.alert}\n",
    "\n",
    "    - `int recvcount`   [number of elements for any single receive]{.alert} (and [not]{.ul}\n",
    "        the total number of items!)\n",
    "\n",
    "    - `MPI_Datatype recvtype`   [data type of received buffer elements]{.alert}\n",
    "\n",
    "    - `int root`   rank of receiving process\n",
    "\n",
    "    - `MPI_Comm comm`   communicator\n",
    "\n",
    "Observe that\n",
    "\n",
    "-   The type signature of `sendcount`, `sendtype` on each\n",
    "    process must be equal to the type signature of `recvcount`,\n",
    "    `recvtype` at all the processes.\n",
    "\n",
    "-   The amount of data sent must be equal to the amount of data\n",
    "    received, pairwise between each process and the root.\n",
    "\n",
    "Therefore, if we need to have a varying count of data from each\n",
    "process, we need to use instead\n",
    "\n",
    "``` c\n",
    "int MPI_Gatherv(const void* sendbuf, int sendcount, \n",
    " MPI_Datatype sendtype, void* recvbuf, \n",
    " const int recvcounts[], const int displs[], \n",
    " MPI_Datatype recvtype, int root, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "- `const int recvcounts[]`   is an array (of length group size) containing the number of elements\n",
    "    that are received from each process,\n",
    "\n",
    "- `const int displs[]`   is an array (of length group size). Entry `i` specifies the\n",
    "    displacement relative to `recvbuf` at which to place the\n",
    "    incoming data from process `i`.\n",
    "\n",
    "If we need to have the\n",
    "result of the *gather* operation on every process involved in the\n",
    "communicator we can use the variant\n",
    "\n",
    "``` c\n",
    "int MPI_Allgather(const void* sendbuf, int sendcount,\n",
    " MPI_Datatype sendtype, void* recvbuf, int recvcount,\n",
    " MPI_Datatype recvtype, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "-   All processes in the communicator `comm` receive the result. The\n",
    "    block of data sent from the $j$th process is received by every\n",
    "    process and placed in the $j$th block of the buffer `recvbuf`.\n",
    "\n",
    "-   The type signature associated with `sendcount`, `sendtype`,\n",
    "    at a process must be equal to the type signature associated with\n",
    "    `recvcount`, `recvtype` at any other process.\n",
    "\n",
    "This function has also the version for gathering messages with\n",
    "different sizes:\n",
    "\n",
    "``` c\n",
    "int MPI_Allgatherv(const void* sendbuf, int sendcount,\n",
    " MPI_Datatype sendtype, void* recvbuf, const int recvcounts[],\n",
    " const int displs[], MPI_Datatype recvtype, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "and works in a way analogous to the `MPI_Gatherv`.\n",
    "\n",
    "The **scatter** is simply the *inverse* operation of `MPI_Gather`\n",
    "\n",
    "``` c\n",
    "int MPI_Scatter(const void* sendbuf, int sendcount, \n",
    "  MPI_Datatype sendtype, void* recvbuf, int recvcount, \n",
    "  MPI_Datatype recvtype, int root, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "- `const void* sendbuf`   [address of send buffer]{.alert}\n",
    "\n",
    "- `int sendcount`   [number of elements sent to each process]{.alert}\n",
    "\n",
    "- `MPI_Datatype sendtype`   [type of send buffer elements]{.alert}\n",
    "\n",
    "- `void* recvbuf`   address of receive buffer\n",
    "\n",
    "- `int recvcount`   number of elements in receive buffer\n",
    "\n",
    "- `MPI_Datatype recvtype`   data type of receive buffer elements\n",
    "\n",
    "- `int root`   rank of sending process\n",
    "\n",
    "- `MPI_Comm comm`   communicator\n",
    "\n",
    "Taxonomy of collective communications: Scatter Observe that\n",
    "\n",
    "-   The type signature of `sendcount`, `sendtype` on each\n",
    "    process must be equal to the type signature of `recvcount`,\n",
    "    `recvtype` at the root.\n",
    "\n",
    "-   The amount of data sent must be equal to the amount of data\n",
    "    received, pairwise between each process and the root.\n",
    "\n",
    "Therefore, if we need to have a varying count of data from each\n",
    "process, we need to use instead\n",
    "\n",
    "``` c\n",
    "int MPI_Scatterv(const void* sendbuf, const int sendcounts[],\n",
    " const int displs[], MPI_Datatype sendtype, void* recvbuf,\n",
    " int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "- `const int sendcounts[]`   is an array (of length group size) containing the number of elements\n",
    "    that are sent to each process,\n",
    "\n",
    "- `const int displs[]`   is an array (of length group size). Entry `i` specifies the\n",
    "    displacement relative to `recvbuf` from which to take the\n",
    "    outgoing data to process `i`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826fc49",
   "metadata": {},
   "source": [
    "### Modifying the 1st derivative code\n",
    "\n",
    "Modifying the 1st derivative code Let us perform the following\n",
    "modification to our first derivative code:\n",
    "\n",
    "1.  Taking from input the number of points to use in each interval,\n",
    "\n",
    "2.  Collecting the whole result on one process and print it on file.\n",
    "\n",
    "For the first step we use the `MPI_Bcast` function,\n",
    "``` c\n",
    "if(mynode == 0){\n",
    " if(argc != 2){\n",
    "  n = 20;\n",
    " }else{\n",
    "  n = atoi(argv[1]);\n",
    " }\n",
    "}\n",
    "MPI_Bcast(&n,1,MPI_INT,\n",
    " 0,MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "-   We read on `rank` $0$ the number `n` from command line,\n",
    "\n",
    "-   Then we broadcast it with `MPI_Bcast`, pay attention to the fact\n",
    "    that the broadcast operations happens on all the processes!\n",
    "    \n",
    "Modifying the 1st derivative code Then we *gather* all the derivatives\n",
    "from the various processes and collect them on process `0`.\n",
    "\n",
    "``` c\n",
    "if(mynode == 0)\n",
    " globalderiv = (double *) \n",
    "   malloc(sizeof(double) \n",
    "   *(n*totalnodes));\n",
    "\n",
    "MPI_Gather(fx,n,MPI_DOUBLE,\n",
    "  globalderiv,n,MPI_DOUBLE,\n",
    "  0,MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "-   we allocate on `rank 0` the memory that is necessary to store\n",
    "    the whole derivative array,\n",
    "\n",
    "-   then we use the to gather all the array `fx` (of `double`)\n",
    "    inside the `globalderiv` array.\n",
    "\n",
    "At last we print it out on file on `rank 0`\n",
    "\n",
    "``` c\n",
    "if(mynode == 0){\n",
    " FILE *fptr; fptr = fopen(\"derivative\", \"w\");\n",
    " for(int i = 0; i < n*totalnodes; i++)\n",
    "  fprintf(fptr,\"%f %f\\n\",globala+i*dx,globalderiv[i]);\n",
    " fclose(fptr); free(globalderiv);}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b31f9",
   "metadata": {},
   "source": [
    "## All-to-All Scatter/Gather\n",
    "\n",
    "All-to-All `MPI_ALLTOALL` is an extension of `MPI_ALLGATHER` to\n",
    "the case where each process sends distinct data to each of the\n",
    "receivers.\n",
    "\n",
    "``` c\n",
    "int MPI_Alltoall(const void* sendbuf, int sendcount,\n",
    " MPI_Datatype sendtype, void* recvbuf, int recvcount, \n",
    " MPI_Datatype recvtype, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "-   The $j$th block sent from process $i$ is received by process $j$ and\n",
    "    is placed in the $i$th block of `recvbuf`.\n",
    "\n",
    "-   The type signature for `sendcount`, `sendtype`, at a process\n",
    "    must be equal to the type signature for `recvcount`,\n",
    "    `recvtype` at any other process.\n",
    "\n",
    "All-to-All different data size If we need to send data of different size\n",
    "between the processes\n",
    "\n",
    "``` c\n",
    "int MPI_Alltoallv(const void* sendbuf, const int sendcounts[],\n",
    " const int sdispls[], MPI_Datatype sendtype, void* recvbuf,\n",
    " const int recvcounts[], const int rdispls[],\n",
    " MPI_Datatype recvtype, MPI_Comm comm);\n",
    "```\n",
    "\n",
    "- `const void* sendbuf`   starting address of send buffer\n",
    "\n",
    "- `const int sendcounts[]`   array specifying the number of elements to send to each rank\n",
    "\n",
    "- `const int sdispls[]`   entry $j$ specifies the displacement (relative to `sendbuf`)\n",
    "    from which to take the outgoing data destined for process $j$\n",
    "\n",
    "- `void* recvbuf`   array specifying the number of elements that can be received from\n",
    "    each rank\n",
    "\n",
    "- `const int recvcounts[]`   integer array. Entry $i$ specifies the displacement (relative to\n",
    "    `recvbuf`) at which to place the incoming data from process $i$\n",
    "\n",
    "- `const int rdispls[]`   entry $i$ specifies the displacement (relative to `recvbuf`) at\n",
    "    which to place the incoming data from process $i$\n",
    "    \n",
    "### Global reduce operation\n",
    "\n",
    "The reduce operation The reduce operation for a given operator takes a\n",
    "data buffer from each of the processes in the communicator group and\n",
    "combines it according to operator rules.\n",
    "\n",
    "``` c\n",
    "int MPI_Reduce(const void* sendbuf, void* recvbuf, \n",
    " int count, MPI_Datatype datatype, MPI_Op op, \n",
    " int root, MPI_Comm comm);\n",
    "```\n",
    "\n",
    "- `const void* sendbuf`   address of send buffer\n",
    "\n",
    "- `void* recvbuf`   address of receive buffer\n",
    "\n",
    "- `int count`   number of elements in send buffer\n",
    "\n",
    "- `MPI_Datatype datatype`   data type of elements of send buffer\n",
    "\n",
    "- `MPI_Op op`   reduce operation\n",
    "\n",
    "- `int root`   rank of root process\n",
    "\n",
    "- `MPI_Comm comm`   communicator\n",
    "\n",
    "The reduce operation The value of `MPI_Op op` for the reduce\n",
    "operation can be taken from any of the following operators.\n",
    "\n",
    "|Constant        | Operation    | Constant      | Operation |\n",
    "|----------------|--------------|---------------|--------------------------|\n",
    "|`MPI_MAX`       | Maximum      |  `MPI_MAXLOC` |  Max value and location|\n",
    "|`MPI_MIN`       | Minimum      |  `MPI_MINLOC` |  Minimum value and location|\n",
    "|`MPI_SUM`       | Sum          |  `MPI_LOR`    |  Logical or|\n",
    "|`MPI_PROD`      | Product      |  `MPI_BOR`    |  Bit-wise or|\n",
    "|`MPI_LAND`      | Logical and  |  `MPI_LXOR`   |  Logical exclusive or|\n",
    "|`MPI_BAND`      | Bit-wise and |  `MPI_BXOR`   |  Bit-wise exclusive or|\n",
    "\n",
    "Moreover, if a different operator is needed, it is possible to\n",
    "create it by means of the function\n",
    "\n",
    "``` c\n",
    "int MPI_Op_create(MPI_User_function* user_fn, int commute, \n",
    "MPI_Op* op)\n",
    "```\n",
    "\n",
    "In C the prototype for a `MPI_User_function` is\n",
    "\n",
    "``` c\n",
    "typedef void MPI_User_function(void* invec, void* inoutvec, \n",
    "    int *len, MPI_Datatype *datatype);\n",
    "```\n",
    "\n",
    "Global reduce operation -- All-Reduce As for other collective operations\n",
    "we may want to have the result of the reduction available on every\n",
    "process in a group.\n",
    "\n",
    "The routine for obtaining such result is\n",
    "\n",
    "``` c\n",
    "int MPI_Allreduce(const void* sendbuf, void* recvbuf, \n",
    " int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "- `const void* sendbuf`   address of send buffer\n",
    "\n",
    "- `void* recvbuf`   address of receive buffer\n",
    "\n",
    "- `int count`   number of elements in send buffer\n",
    "\n",
    "- `MPI_Datatype datatype`   data type of elements of send buffer\n",
    "\n",
    "- `MPI_Op op`   reduce operation\n",
    "\n",
    "- `MPI_Comm comm`   communicator\n",
    "\n",
    "This instruction behaves like a combination of a *reduction* and\n",
    "*broadcast* operation.\n",
    "\n",
    "Global reduce operation -- All-Reduce-Scatter This is another variant of\n",
    "the reduction operation in which the result is *scattered* to all\n",
    "processes in a group on return.\n",
    "\n",
    "``` c\n",
    "int MPI_Reduce_scatter_block(const void* sendbuf, \n",
    " void* recvbuf, int recvcount, MPI_Datatype datatype, \n",
    " MPI_Op op, MPI_Comm comm);\n",
    "```\n",
    "\n",
    "-   The routine is called by all group members using the same arguments\n",
    "    for `recvcount`, `datatype`, `op` and `comm`.\n",
    "\n",
    "-   The resulting vector is treated as `n` consecutive blocks of\n",
    "    `recvcount` elements that are scattered to the processes of the\n",
    "    group `comm`.\n",
    "\n",
    "-   The $i$th block is sent to process $i$ and stored in the receive\n",
    "    buffer defined by `recvbuf`, `recvcount`, and\n",
    "    `datatype`.\n",
    "\n",
    "Global reduce operation -- All-Reduce-Scatter Of this function also a\n",
    "variant with variable block--size is available\n",
    "\n",
    "``` c\n",
    "int MPI_Reduce_scatter(const void* sendbuf, void* recvbuf,\n",
    "const int recvcounts[], MPI_Datatype datatype, MPI_Op op,\n",
    "MPI_Comm comm);\n",
    "```\n",
    "\n",
    "-   This routine first performs a global element-wise reduction on\n",
    "    vectors of $\\verb|count|=\\sum_{i=0}^{n-1}\\verb|recevcounts[i]|$\n",
    "    elements in the send buffers defined by `sendbuf`, `count`\n",
    "    and `datatype`, using the operation `op`, where `n` is\n",
    "    the size of the communicator.\n",
    "\n",
    "-   The routine is called by all group members using the same arguments\n",
    "    for `recvcounts`, `datatype`, `op` and `comm`.\n",
    "\n",
    "-   The resulting vector is treated as `n` consecutive blocks where\n",
    "    the number of elements of the $i$th block is `recvcounts[i]`.\n",
    "\n",
    "-   The $i$th block is sent to process $i$ and stored in the receive\n",
    "    buffer defined by `recvbuf`, `recvcounts[i]` and\n",
    "    `datatype`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7858ddb",
   "metadata": {},
   "source": [
    "# Some computations using collective communications\n",
    "\n",
    "## Computing Integrals\n",
    "For an\n",
    "integrable function $f : [a,b] \\rightarrow \\mathbb{R}$ the *midpoint*\n",
    "rule (sometimes *rectangle* rule) is given by\n",
    "$$\\int_{a}^{b}f(x) dx \\approx I_1 = (b-a) f\\left(\\frac{a+b}{2}\\right),$$\n",
    "\n",
    "This is a very crude approximation, to make it more accurate we may\n",
    "break up the interval $[a,b]$ into a number $n$ of non-overlapping\n",
    "subintervals $[a_k,b_k]$ such that $[a,b] = \\cup_k [a_k,b_k]$,\n",
    "$$I_n =  \\sum_{k=0}^n(b_k-a_k) f\\left(\\frac{a_k+b_k}{2}\\right)$$\n",
    "\n",
    "Computing integrals with parallel midpoint quadrature rule If we want to\n",
    "transform this computation in a parallel computation we can adopt the\n",
    "following sketch:\n",
    "\n",
    "1.  `if (mynode == 0)` get number of intervals for quadrature\n",
    "\n",
    "2.  broadcast number of intervals to all the processes\n",
    "\n",
    "3.  assign the non-overlapping intervals to the processes\n",
    "\n",
    "4.  sum function values in the center of each interval\n",
    "\n",
    "5.  reduce with operator sum the integral on process 0.\n",
    "\n",
    "As a test function for the parallel integration routine we can use\n",
    "$$f(x) = \\frac{4}{1+x^2}; \\qquad I = \\int_{0}^{1} \\frac{4}{1+x^2} dx = \\pi.$$\n",
    "To evaluate the error we can use the value :\n",
    "`double PI25DT = 3.141592653589793238462643;`\n",
    "\n",
    "``` c\n",
    "h   = 1.0 / ((double) n*totalnodes);\n",
    "sum = 0.0;\n",
    "for (i = 1+mynode*n;\n",
    "    i <= n*(mynode+1);\n",
    "    i++){\n",
    " x = h * ((double)i - 0.5);\n",
    " sum += f(x);\n",
    "}\n",
    "mypi = h * sum;\n",
    "MPI_Reduce(&mypi, &pi, 1, \n",
    "    MPI_DOUBLE, \n",
    "    MPI_SUM, 0, \n",
    "    MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "-   We assume that all the intervals have the same size, thus the\n",
    "    scaling `h   = 1.0 / (double) n`,\n",
    "\n",
    "-   We compute all the value $x$ that are in the local process and\n",
    "    increment the local sum,\n",
    "\n",
    "-   in conclusion we perform an `MPI_Reduce` to sum together all the\n",
    "    local sums.\n",
    "\n",
    "You can then print out the obtained value of $\\pi$ and the error with\n",
    "respect to `PI25DT` as\n",
    "\n",
    "``` c\n",
    "if (mynode == 0){\n",
    " printf(\"pi is approximately %.16f, Error is %e\\n\",\n",
    "    pi, fabs(pi - PI25DT));\n",
    "}\n",
    "```\n",
    "\n",
    "### Python version\n",
    "\n",
    "We can implement the same algorithm using MPI4PY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file quadrature.py\n",
    "\"\"\"\n",
    "Computing pi with parallel quadrature formula\n",
    "\"\"\"\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "mynode = comm.Get_rank()\n",
    "totalnodes = comm.Get_size()\n",
    "\n",
    "def fun(x):\n",
    "    return (4.0/(1.0 + x*x))\n",
    "\n",
    "truepi = 3.141592653589793238462643\n",
    "dest = 0\n",
    "pi = numpy.zeros(1)\n",
    "\n",
    "# Initialize value of n only if this is rank 0\n",
    "# we are not checking the input, if the user does something that\n",
    "# has no meaning we are going to fail badly!\n",
    "if mynode == 0:\n",
    "    if len(sys.argv) == 1:\n",
    "        n = numpy.full(1, 20, dtype=int) # default value\n",
    "    else:\n",
    "        n = numpy.full(1,int(sys.argv[1]),dtype=int)\n",
    "else:\n",
    "    n = numpy.zeros(1, dtype=int)\n",
    "\n",
    "# Broadcast n to all processes\n",
    "comm.Bcast(n, root=0)\n",
    "\n",
    "# Compute local integral\n",
    "my_pi = numpy.zeros(1)\n",
    "h = 1.0/(n*totalnodes)\n",
    "for i in numpy.arange(1+mynode*n,n*(mynode+1)+1):\n",
    "    x = h*(i - 0.5)\n",
    "    my_pi = my_pi + fun(x)    \n",
    "my_pi = h*my_pi\n",
    "\n",
    "# Send partition back to root process:\n",
    "comm.Reduce(my_pi, pi, MPI.SUM, dest)\n",
    "\n",
    "# Only print the result in process 0\n",
    "if mynode == 0:\n",
    "    print('The Integral Sum =', pi[0],\" The Error is \",numpy.abs(pi[0]-truepi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c920c8",
   "metadata": {},
   "source": [
    "That we can execute as usual by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76931e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python quadrature.py 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf316e81",
   "metadata": {},
   "source": [
    "## Random number generation: Montecarlo type algorithms\n",
    "\n",
    "Montecarlo methods are algorithms that\n",
    "rely on a procedure of repeated random sampling to obtain numerical\n",
    "results[^1].\n",
    "\n",
    "A generic Montecarlo algorithm can be described by the following 4\n",
    "steps\n",
    "\n",
    "1.  define a domain of possible samples\n",
    "\n",
    "2.  generate the samples from a probability distribution over such\n",
    "    domain\n",
    "\n",
    "3.  perform a deterministic computation on the inputs\n",
    "\n",
    "4.  aggregate the results\n",
    "\n",
    "We can write the parallel version of\n",
    "such algorithm in the following way\n",
    "\n",
    "1.  we divide a square in an number of parts equal to the number of\n",
    "    processes we have,\n",
    "\n",
    "2.  we generate a number of random points $(x,y)$ in the area owned by\n",
    "    each process,\n",
    "\n",
    "3.  we compute how many points fall in the circle\n",
    "\n",
    "4.  sum-reduce the number of points in the square and in the circle\n",
    "\n",
    "5.  divide the two numbers on process 0 to get the approximation\n",
    "\n",
    "We can generate on each node the\n",
    "sampling on the reference square by\n",
    "\n",
    "``` c\n",
    "h  = 2.0 / (double) totalnodes;\n",
    "x1 = -1.0 + mynode * h;\n",
    "x2 = x1 + h;\n",
    "y1 = -1.0;\n",
    "y2 = 1.0;\n",
    "my_SqPoints  = 0;\n",
    "my_CiPoints  = 0;\n",
    "\n",
    "for (i = 1; i <= n; i += totalnodes){\n",
    " x = rand(); x = x / RAND_MAX; x = x1 + x * (x2 - x1);\n",
    " y = rand(); y = y / RAND_MAX; y = y1 + y * (y2 - y1);\n",
    " my_SqPoints++;\n",
    " if ( ( x*x + y*y ) <= 1.0 ) my_CiPoints++;\n",
    "}\n",
    "```\n",
    "\n",
    "Then we perform the reduction by doing\n",
    "\n",
    "``` c\n",
    "SqPoints = 0;\n",
    "CiPoints = 0;\n",
    "MPI_Reduce(&my_SqPoints, &SqPoints, 1, MPI_INT, MPI_SUM, 0,\n",
    "    MPI_COMM_WORLD);\n",
    "MPI_Reduce(&my_CiPoints, &CiPoints, 1, MPI_INT, MPI_SUM, 0,\n",
    "    MPI_COMM_WORLD);\n",
    "```\n",
    "\n",
    "and print the approximation\n",
    "\n",
    "``` c\n",
    "if (mynode == 0){\n",
    "pi = 4.0 * (double)CiPoints / (double)SqPoints;\n",
    "printf(\"Pi is approximately %.16f, Error is %e\\n\"\n",
    "    ,pi, fabs(pi - PI25DT));\n",
    "}\n",
    "```\n",
    "\n",
    "[^1]: For some historical information about this idea:\n",
    "    <http://shorturl.at/mAWY8>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5269ad",
   "metadata": {},
   "source": [
    "### Python version\n",
    "\n",
    "Again we can implemente the same procedure in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file montecarlo.py\n",
    "\"\"\"\n",
    "Computing pi with parallel quadrature formula\n",
    "\"\"\"\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import random\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "mynode = comm.Get_rank()\n",
    "totalnodes = comm.Get_size()\n",
    "\n",
    "truepi = 3.141592653589793238462643\n",
    "dest = 0\n",
    "pi = numpy.zeros(1)\n",
    "\n",
    "# Initialize value of n only if this is rank 0\n",
    "# we are not checking the input, if the user does something that\n",
    "# has no meaning we are going to fail badly!\n",
    "if mynode == 0:\n",
    "    if len(sys.argv) == 1:\n",
    "        n = numpy.full(1, 20, dtype=int) # default value\n",
    "    else:\n",
    "        n = numpy.full(1,int(sys.argv[1]),dtype=int)\n",
    "else:\n",
    "    n = numpy.zeros(1, dtype=int)\n",
    "\n",
    "# Broadcast n to all processes\n",
    "comm.Bcast(n, root=0)\n",
    "\n",
    "# Compute local quantities\n",
    "h  = 2.0 / totalnodes\n",
    "x1 = -1.0 + mynode * h\n",
    "x2 = x1 + h\n",
    "y1 = -1.0\n",
    "y2 = 1.0\n",
    "my_SqPoints  = numpy.zeros(1, dtype=int)\n",
    "my_CiPoints  = numpy.zeros(1, dtype=int)\n",
    "\n",
    "for i in numpy.arange(1,n+1,totalnodes):\n",
    "    x = random.random()\n",
    "    x = x1 + x*(x2-x1)\n",
    "    y = random.random()\n",
    "    y = y1 + y*(y2-y1)\n",
    "    my_SqPoints = my_SqPoints + 1\n",
    "    if ( x**2 + y**2 <= 1.0 ):\n",
    "        my_CiPoints = my_CiPoints + 1\n",
    "    \n",
    "SqPoints = numpy.zeros(1, dtype=int)\n",
    "CiPoints = numpy.zeros(1, dtype=int)\n",
    "\n",
    "# Send back to the root process the data\n",
    "comm.Reduce(my_SqPoints, SqPoints, MPI.SUM, dest)\n",
    "comm.Reduce(my_CiPoints, CiPoints, MPI.SUM, dest)\n",
    "\n",
    "# Only print the result in process 0\n",
    "if mynode == 0:\n",
    "    pi = 4.0*float(CiPoints)/float(SqPoints)\n",
    "    print('The Integral Sum =', pi,\" The Error is \",numpy.abs(pi-truepi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19a3fc",
   "metadata": {},
   "source": [
    "That we can execute as usual by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ac314",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python montecarlo.py 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb772e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
