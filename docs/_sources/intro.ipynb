{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dc9eff",
   "metadata": {},
   "source": [
    "# Message Passing Interface\n",
    "\n",
    "How do we realize practically this parallelism?\n",
    "\n",
    "Let us focus on what we have discussed until now:\n",
    "* We have ``machines'' with multiple processors and whose main memory is partitioned into fragmented components,\n",
    "* We have algorithms that can divide a problem of size $N$ among these processors so that they can run (almost) independently,\n",
    "* With a certain degree of approximation, we know how to compute what is the *best improvement* we can expect from a parallel program with $M$ processors on a problem of size $N$.\n",
    "\n",
    "What we need to discuss now is then: \"*How can we actually implement these algorithms on real machines?*\"\n",
    "* We need a way to define a parallel environment in which every processor is accounted for,\n",
    "* We need to have data formats that are aware of the fact that we have a *distributed* memory,\n",
    "* We need to exchange data between the various memory fragments.\n",
    "\n",
    ">\"MPI (Message Passing Interface) is a **specification for a standard library** for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists.\" -- W. Gropp, E. Lusk, N. Doss, A. Skjellum,'' -- A high-performance, portable implementation of the MPI message passing interface standard, Parallel Computing, 22 (6), 1996.\n",
    "\n",
    "* MPI implementations consist of a specific set of routines directly callable from C, C++, Fortran, Python;\n",
    "* MPI uses Language Independent Specifications for calls and language bindings;\n",
    "* The MPI interface provides an essential *virtual* topology, synchronization, and communication functionality inside a set of processes.\n",
    "* There exist many implementations of the MPI specification, e.g., MPICH, Open MPI, *etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0c014",
   "metadata": {},
   "source": [
    "## Our First MPI Program\n",
    "\n",
    "In all the course we are going to use the MPI inside Python programs.\n",
    "\n",
    "Let us start from the classical helloworld program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174cd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/helloworld.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/helloworld.c\n",
    "#include\"mpi.h\"\n",
    "#include<stdio.h>\n",
    "\n",
    "int main(int argc,char **argv){\n",
    " MPI_Init( &argc, &argv);\n",
    " printf(\"Hello, world!\\n\");\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbf39e",
   "metadata": {},
   "source": [
    "We can compile it by doing\n",
    "``` bash\n",
    "mpicc helloworld.c -o helloworld\n",
    "```\n",
    "- `mpicc` is a wrapper for a C compiler provided by the Open MPI implementation of MPI.\n",
    "- the option `-o` sets the name of the compiled (executable) file.\n",
    "\n",
    "Let us see what is happening behind the curtains\n",
    "- you can first try to discover what compiler are you using by executing \n",
    "```bash\n",
    "mpicc --version\n",
    "```\n",
    "that will give you something like\n",
    "```\n",
    "gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n",
    "Copyright (C) 2017 Free Software Foundation, Inc.\n",
    "```\n",
    "- or discover what are the library inclusion and linking options by asking for `mpicc --showme:compile` and `mpicc --showme:link`, respectively.\n",
    "- In general, looking at the output of the `man mpicc` command is always a good idea.\n",
    "\n",
    "> ``If you find yourself saying, \"But I don't want to use wrapper compilers!\", please humor us and try them. See if they work for you. Be sure to let us know if they do not work for you. '' - [https://www.open-mpi.org/faq/?category=mpi-apps](https://www.open-mpi.org/faq/?category=mpi-apps)\n",
    "\n",
    "```{note}\n",
    "A piece of advice: if your program is anything more realistic than a classroom exercise use `make`[^1], and save yourself from writing painfully long compiling commands, and dealing with complex dependencies more than once.\n",
    "```\n",
    "\n",
    "> \"Make gets its knowledge of how to build your program from a file called the makefile, which lists each of the non-source files and how to compute it from other files.\"\n",
    "\n",
    "A very simple `Makefile` for our first test would be\n",
    "``` make\n",
    "MPICC = mpicc #The wrapper for the compiler\n",
    "CFLAGS += -g  #Useful for debug symbols\n",
    "all: helloworld\n",
    "helloworld: helloworld.c\n",
    "  $(MPICC) $(CFLAGS) $(LDFLAGS) $? $(LDLIBS) -o $@\n",
    "clean:\n",
    "  rm -f helloworld\n",
    "```\n",
    "\n",
    "[^1]:[https://www.gnu.org/software/make/](https://www.gnu.org/software/make/)\n",
    "\n",
    "Let us run our first parallel program by doing:\n",
    "```bash\n",
    "mpirun [ -np X ] [ --hostfile <filename> ]  python helloworld.py\n",
    "```\n",
    "or by using its synonym\n",
    "```bash\n",
    "mpiexec [ -np X ] [ --hostfile <filename> ] python helloworld.py\n",
    "```\n",
    "* `mpiexec` will  run  `X` copies of `helloworld` in your current run-time environment, scheduling (by default) in a round-robin fashion by CPU slot.\n",
    "* if running under a supported resource manager, Open MPI's `mpirun` will usually automatically use the corresponding resource manager process starter, as opposed to, for example, rsh or ssh, which require the use of a hostfile, or will default  to  running all `X` copies on the localhost \n",
    "* as always, look at the manual, by doing `man mpirun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1de7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cirdan/anaconda3/envs/parallel/bin/x86_64-conda_cos6-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/cirdan/anaconda3/envs/parallel/include -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib  helloworld.c   -o helloworld\n",
      "/home/cirdan/anaconda3/envs/parallel/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: /tmp/cc0XAkgc.o: in function `main':\n",
      "helloworld.c:(.text.startup.main+0x16): undefined reference to `MPI_Init'\n",
      "/home/cirdan/anaconda3/envs/parallel/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: helloworld.c:(.text.startup.main+0x29): undefined reference to `MPI_Finalize'\n",
      "collect2: error: ld returned 1 exit status\n",
      "<incorporato>: recipe for target 'helloworld' failed\n",
      "make: *** [helloworld] Error 1\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make helloworld)\n",
    "!mpiexec -np 4 ./ccode/helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd1d38",
   "metadata": {},
   "source": [
    "Every process executes the line that it is a **local** routine!\n",
    "\n",
    "> A procedure is **local** if completion\n",
    "of the procedure depends only on the local executing process.\n",
    "\n",
    ">A procedure is **non-local** if completion of the operation may require\n",
    "the execution of some MPI procedure on another process. Such an\n",
    "operation *may require communication* occurring with another user\n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ef55a",
   "metadata": {},
   "source": [
    "## The MPI parallel environment\n",
    "\n",
    "The MPI parallel environment Let us modify our `helloworld` to\n",
    "investigate the MPI parallel environment. Specifically, we want to\n",
    "answer, from within the program, to the questions:\n",
    "\n",
    "1.  How many processes are there?\n",
    "\n",
    "2.  Who am I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5703f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/hamlet.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/hamlet.c\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "int main( int argc, char **argv ){\n",
    " int rank, size;\n",
    " MPI_Init( &argc, &argv );\n",
    " MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n",
    " MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    " printf( \"Hello world! I'm process %d of %d\\n\",rank, size );\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401eb7f",
   "metadata": {},
   "source": [
    "-   How many is answered by a call to `MPI_Comm_size` as an\n",
    "    `int` value,\n",
    "\n",
    "-   Who am I? Is answered by a call to `MPI_Comm_rank` as an\n",
    "    `int` value that is conventionally called `rank` and is a\n",
    "    number between `0` and `size-1`.\n",
    "\n",
    "The MPI parallel environment The last keyword we need to describe is the\n",
    "`MPI_COMM_WORLD`, this is the standard Communicator object.\n",
    "\n",
    "> **Communicator:** A Communicator object connects a group of processes in one\n",
    "MPI session. There can be more than one communicator in an MPI session,\n",
    "each of them gives each contained process an independent identifier and\n",
    "arranges its contained processes in an ordered topology.\n",
    "\n",
    "This provides\n",
    "\n",
    "-   a safe communication space, that guarantees that the code can\n",
    "    communicate as they need to, without conflicting with communication\n",
    "    extraneous to the present code, e.g., if other parallel libraries\n",
    "    are in use,\n",
    "\n",
    "-   a unified object for conveniently denoting communication context,\n",
    "    the group of communicating processes and to house abstract process\n",
    "    naming.\n",
    "\n",
    "The MPI parallel environment If we have saved our inquiring MPI program\n",
    "in the file `hamlet.c`, we can then modify our `Makefile`\n",
    "by modifying/adding the lines\n",
    "\n",
    "``` Makefile\n",
    "all: helloworld hamlet\n",
    "hamlet: hamlet.c\n",
    " $(MPICC) $(CFLAGS) $(LDFLAGS) $? $(LDLIBS) -o $@\n",
    "clean:\n",
    " rm -f helloworld hamlet\n",
    "```\n",
    "\n",
    "Then, we compile everything by doing `make hamlet` (or, simply,\n",
    "`make`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9187fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc\t\t\t -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -g\t\t\t -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib hamlet.c  -o hamlet\n",
      "Hello world! I'm process 0 of 6\n",
      "Hello world! I'm process 1 of 6\n",
      "Hello world! I'm process 2 of 6\n",
      "Hello world! I'm process 5 of 6\n",
      "Hello world! I'm process 3 of 6\n",
      "Hello world! I'm process 4 of 6\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make hamlet)\n",
    "!mpiexec -np 6 ./ccode/hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2f24e",
   "metadata": {},
   "source": [
    "We can rewrite the same code in Python as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca81a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hamlet.py\n"
     ]
    }
   ],
   "source": [
    "%%file hamlet.py\n",
    "\"\"\"\n",
    "Hello (parallel) world!\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "size = comm.Get_size() \n",
    "\n",
    "print(\"Hello world! I'm process \",rank,\" of \",size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2c6e3",
   "metadata": {},
   "source": [
    "What have we done here:\n",
    "* The instruction \n",
    "```python\n",
    "from mpi4py import MPI\n",
    "```\n",
    "provides basic MPI definitions and types, if this was a `C` code, this would have been a *preprocessor* directive of the form `#include \"mpi.h\"`\n",
    "* start MPI by creating a communicator \n",
    "```python\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "\n",
    "For the Python code\n",
    "\n",
    "* How many is answered by a call to `comm.Get_size()` as an `int` value,\n",
    "* Who am I? Is answered by a call to `comm.Get_rank()` as an `int` value that is conventionally called **rank** and is a number between `0` and `size-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4eecd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I'm process  0  of  4\n",
      "Hello world! I'm process  1  of  4\n",
      "Hello world! I'm process  2  of  4\n",
      "Hello world! I'm process  3  of  4\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python hamlet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92542db",
   "metadata": {},
   "source": [
    "* Every processor answers the call,\n",
    "* But it answers it as soon as he has done doing the computation! There is **no synchronization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a6771",
   "metadata": {},
   "source": [
    "# Point-to-point communication\n",
    "\n",
    "Sending and Receiving Messages We have seen that each process within a\n",
    "*communicator* is identified by its *rank*, how can we [exchange\n",
    "data]{.alert} between two processes?\n",
    "\n",
    "We need to posses several information to have a meaningful message\n",
    "\n",
    "-   Who is sending the data?\n",
    "\n",
    "-   To whom the data is sent?\n",
    "\n",
    "-   What type of data are we sending?\n",
    "\n",
    "-   How does the receiver can identify it?\n",
    "\n",
    "## The blocking send and receive\n",
    "\n",
    "```c\n",
    "int MPI_Send(void *message, int count, \n",
    "    MPI_Datatype datatype, int dest, int tag, \n",
    "    MPI_Comm comm)\n",
    "```\n",
    "\n",
    "- `void *message`:   points to the message content itself, it can be a simple scalar or a\n",
    "    group of data,\n",
    "\n",
    "- `int count`:   specifies the number of data elements of which the message is\n",
    "    composed,\n",
    "\n",
    "- `MPI_Datatype datatype`:   indicates the [data type]{.alert} of the elements that make up the\n",
    "    message,\n",
    "\n",
    "- `int dest`:   the rank of the destination process,\n",
    "\n",
    "- `int tag`:   the user-defined tag field,\n",
    "\n",
    "- `MPI_Comm comm`:   the communicator in which the source and destination processes\n",
    "    reside and for which their respective ranks are defined.\n",
    "\n",
    "```c\n",
    "int MPI_Recv (void *message, int count, \n",
    "    MPI_Datatype datatype, int source, int tag,\n",
    "    MPI_Comm comm, MPI_Status *status)\n",
    "```\n",
    "\n",
    "- `void *message`:   points to the message content itself, it can be a simple scalar or a\n",
    "    group of data,\n",
    "\n",
    "- `int count`:   specifies the number of data elements of which the message is\n",
    "    composed,\n",
    "\n",
    "- `MPI_Datatype datatype`:   indicates the [data type]{.alert} of the elements that make up the\n",
    "    message,\n",
    "\n",
    "- `int dest`:   the rank of the source process,\n",
    "\n",
    "- `int tag`:   the user-defined tag field,\n",
    "\n",
    "- `MPI_Comm comm`:   the communicator in which the source and destination processes\n",
    "    reside,\n",
    "\n",
    "- `MPI_Status *status`:   is a structure that contains three fields named `MPI_SOURCE` ,\n",
    "    `MPI_TAG`, and `MPI_ERROR`.\n",
    "\n",
    "\n",
    "Basic MPI Data Types Of the inputs in the previous slides the only one\n",
    "that is specific to MPI is the `MPI_Datatype`, these corresponds to\n",
    "a C data type\n",
    "\n",
    "MPI Data Types | C Type\n",
    "---------------------------|-------------------------\n",
    "`MPI_CHAR`             |`signed char`\n",
    "`MPI_SHORT`            |`signed short int`\n",
    "`MPI_INT`              |`signed int`\n",
    "`MPI_LONG`             |`signed long int`\n",
    "`MPI_FLOAT`            |`float`\n",
    "`MPI_DOUBLE`           |`double`\n",
    "`MPI_LONG_DOUBLE`      |`long double`\n",
    "`MPI_UNSIGNED_CHAR`    |`unsigned char`\n",
    "`MPI_UNSIGNED_SHORT`   |`unsigned short int`\n",
    "`MPI_UNSIGNED`         |`unsigned int`\n",
    "`MPI_UNSIGNED_LONG`    |`unsigned long int`\n",
    "\n",
    "**Note:** we will see in the following how to\n",
    "`send`/`receive` user--defined data structures.\n",
    "\n",
    "Why \"blocking\" send and receive? For the `MPI_Send` to be\n",
    "**blocking** means that it does not return until the message data\n",
    "and envelope have been safely stored away so that the sender is free to\n",
    "modify the send buffer: it is a *non local* operation.\n",
    "\n",
    "**Note:** The message might be copied directly into the\n",
    "matching receive buffer (as in the first figure), or it might be copied\n",
    "into a temporary system buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02b066",
   "metadata": {},
   "source": [
    "## A simple send/receive example\n",
    "\n",
    "If we want to test these two instructions we can write the following simple C program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0194d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/easysendrecv.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/easysendrecv.c\n",
    "#include \"mpi.h\"\n",
    "#include <string.h>\n",
    "#include <stdio.h>\n",
    "int main( int argc, char **argv){\n",
    " char message[20];\n",
    " int myrank;\n",
    " MPI_Status status;\n",
    " MPI_Init( &argc, &argv );\n",
    " MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n",
    " if (myrank == 0){  /* code for process zero */\n",
    "  strcpy(message,\"Hello, there\");\n",
    "  MPI_Send(message, strlen(message)+1, MPI_CHAR, 1, 99, MPI_COMM_WORLD);\n",
    " }\n",
    " else if (myrank == 1){ /* code for process one */\n",
    "  MPI_Recv(message, 20, MPI_CHAR, 0, 99, MPI_COMM_WORLD, &status);\n",
    "  printf(\"received :%s:\\n\", message);\n",
    " }\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0377ed",
   "metadata": {},
   "source": [
    "That could be recasted in Python by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c74878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting easysendrecv.py\n"
     ]
    }
   ],
   "source": [
    "%%file easysendrecv.py\n",
    "\"\"\"\n",
    "A simple send/receive example\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "size = comm.Get_size() \n",
    "\n",
    "if rank == 0:\n",
    "    data = \"Hello, there\"\n",
    "    comm.send(data, dest=1, tag=99)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=99)\n",
    "    print('received :',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ca205",
   "metadata": {},
   "source": [
    "That we can run as the simpler program by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f54c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received : Hello, there\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 2 python easysendrecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436b2d9",
   "metadata": {},
   "source": [
    "for the Python version, or the following for the C version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f52730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc\t\t\t -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -g\t\t\t -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib easysendrecv.c  -o easysendrecv\n",
      "received :Hello, there:\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make easysendrecv)\n",
    "!mpiexec -np 2 ./ccode/easysendrecv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c2239",
   "metadata": {},
   "source": [
    "So, what have we done? Process $0$ sends the content of the `char`\n",
    "array `message[20]`, whose size is `strlen(message)+1` size of\n",
    "`char` (`MPI_CHAR`) to processor `1` with tag `99` on\n",
    "the communicator `MPI_COMM_WORLD`. on the other side process $1$,\n",
    "receives into the buffer `message[20]` an array with size `20`\n",
    "size of `MPI_CHAR`, from process `0` with tag `99` on the\n",
    "same communicator `MPI_COMM_WORLD`.\n",
    "\n",
    "Observe that in the Python case we did not declare the size or the type of the object we were passing. The all-lowercase methods (of the `Comm` class), like `send()`, `recv()`, work by passing an object to be sent as a paramenter to the communication call, and the received object is simply the return value. These variants can communicate general Python objects.\n",
    "\n",
    "In MPI for Python, the `MPI.Comm.Send()`, `MPI.Comm.Recv()` and methods of communicator objects provide support for blocking point-to-point communications and can be used to communicate memory buffers, as we do in the C variant. Consider the following example sending a `numpy` array between two processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b53a550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting easysendrecv2.py\n"
     ]
    }
   ],
   "source": [
    "%%file easysendrecv2.py\n",
    "\"\"\"\n",
    "A (slightly less) simple send/receive example\n",
    "In which we :\n",
    "- pass MPI datatypes explicitly\n",
    "- use the MPI datatype discovery\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = numpy.arange(1000, dtype='i')\n",
    "    comm.Send([data, MPI.INT], dest=1, tag=77)\n",
    "elif rank == 1:\n",
    "    data = numpy.empty(1000, dtype='i')\n",
    "    comm.Recv([data, MPI.INT], source=0, tag=77)\n",
    "\n",
    "if rank == 0:\n",
    "    data = numpy.arange(100, dtype=numpy.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "elif rank == 1:\n",
    "    data = numpy.empty(100, dtype=numpy.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ade2c5",
   "metadata": {},
   "source": [
    "In general, buffer arguments to these calls must be explicitly specified by using a 2/3-list/tuple like `[data, MPI.DOUBLE]`, or `[data, count, MPI.DOUBLE]` (the former one uses the byte-size of data and the extent of the MPI datatype to define count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3170e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python easysendrecv2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439bde5",
   "metadata": {},
   "source": [
    "The perform the datatype discovery, Python uses the `pickle` module. This module implements binary protocols for serializing and de-serializing a Python object structure. \"Pickling\" is the process converting a Python object hierarchy into a byte stream, and \"unpickling\" is the inverse operation, converting a byte stream (from a binary file or bytes-like object) into an object hierarchy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3862ba",
   "metadata": {},
   "source": [
    "### A simple send/receive example : programmer smash!\n",
    "\n",
    "It is a good exercise\n",
    "to try and mess things up, so let us see some damaging suggestions (test them with the previous C code):\n",
    "\n",
    "-   What happens if we have a mismatch in the tags?\n",
    "\n",
    "-   **A:** The process stays there hanging waiting for a message with a tag\n",
    "    that will never come...\n",
    "\n",
    "-   What happens if we have a mismatch in the ranks of the sending and\n",
    "    receiving processes?\n",
    "\n",
    "-    **A:** The process stays there hanging trying to match messages that will\n",
    "    never come...\n",
    "\n",
    "-   What happens if we use the wrong message size?\n",
    "\n",
    "-    **A:** If the size of the arriving message is longer than the expected we\n",
    "    get an error of `MPI_ERR_TRUNCATE: message truncated`, note\n",
    "    that there are combinations of wrong sizes for which things still\n",
    "    works\n",
    "\n",
    "-   What happens if we have a mismatch in the type?\n",
    "\n",
    "-   **A:** There are combinations of instances in which things seems to work,\n",
    "    **but** the code is erroneous, and the behavior is not\n",
    "    deterministic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a04833",
   "metadata": {},
   "source": [
    "## Deadlock\n",
    "\n",
    "We have now two processes that needs to exchange some data.\n",
    "\n",
    "-   Solution 1:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 1, tag, comm);\n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 1, tag, comm, status);\n",
    "}else if(myrank == 1){\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 0, tag, comm); \n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 0, tag, comm, status);\n",
    "}\n",
    "```\n",
    "\n",
    "-   Solution 2:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 1, tag, comm, status);\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 1, tag, comm);\n",
    "}else if(myrank == 1){ \n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 0, tag, comm, status);\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 0, tag, comm);\n",
    "}\n",
    "```\n",
    "\n",
    "-   Solution 3:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 1, tag, comm);\n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 1, tag, comm, status);\n",
    "}else if(myrank == 1){\n",
    " MPI_Recv(recvbuf, count, MPI_DOUBLE, 0, tag, comm, status);\n",
    " MPI_Send(sendbuf, count, MPI_DOUBLE, 0, tag, comm); \n",
    "}\n",
    "```\n",
    "\n",
    "In the case of Solution 1:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Send(...);\n",
    " MPI_Recv(...);\n",
    "}else if(myrank == 1){\n",
    " MPI_Send(...); \n",
    " MPI_Recv(...);\n",
    "}\n",
    "```\n",
    "\n",
    "-   The call `MPI_Send` is blocking, therefore the message sent by\n",
    "    each process has to be copied out before the send operation returns\n",
    "    and the receive operation starts.\n",
    "\n",
    "-   For the call to complete successfully, it is then necessary that **at\n",
    "    least one of the two messages sent be buffered**, otherwise\n",
    "    ...\n",
    "\n",
    "-   a deadlock situation occurs: both processes are blocked since there\n",
    "    is no buffer space available!\n",
    "\n",
    "In the case of Solution 2:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Recv(...);\n",
    " MPI_Send(...);\n",
    "}else if(myrank == 1){\n",
    " MPI_Recv(...);\n",
    " MPI_Send(...); \n",
    "}\n",
    "```\n",
    "\n",
    "-   The receive operation of process $0$ must complete before its send.\n",
    "    It can complete **only if** the matching send of processor $1$\n",
    "    is executed.\n",
    "\n",
    "-   The receive operation of process $1$ must complete before its send.\n",
    "    It can complete **only if** the matching send of processor $0$\n",
    "    is executed.\n",
    "\n",
    "-   This program will always deadlock.\n",
    "\n",
    "In the case of Solution 3:\n",
    "\n",
    "``` c\n",
    "MPI_Comm_rank(comm, &myrank);\n",
    "if (myrank == 0){\n",
    " MPI_Send(...);\n",
    " MPI_Recv(...);\n",
    "}else if(myrank == 1){\n",
    " MPI_Recv(...);\n",
    " MPI_Send(...); \n",
    "}\n",
    "```\n",
    "\n",
    "-   This program will succeed even if no buffer space for data is\n",
    "    available.\n",
    "    \n",
    "### Deadlock Issues\n",
    "\n",
    "We can try to salvage what the situation in the case of Solution 1 by\n",
    "allocating the buffer space for the send calls\n",
    "\n",
    "``` c\n",
    "if (myrank == 0){\n",
    " MPI_Send(...);\n",
    " MPI_Recv(...);\n",
    "}else if(myrank == 1){\n",
    " MPI_Send(...); \n",
    " MPI_Recv(...);\n",
    "}\n",
    "```\n",
    "\n",
    "We can substitute the `MPI_Send` operation with a Send in\n",
    "buffered mode\n",
    "\n",
    "``` c\n",
    "int MPI_Bsend(const void* buf, int count, \n",
    "  MPI_Datatype datatype, int dest,\n",
    "  int tag, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "-   A buffered mode send operation can be started whether or not a\n",
    "    matching receive has been posted;\n",
    "\n",
    "-   It may complete before a matching receive is posted;\n",
    "\n",
    "-   This operation is *local*!\n",
    "\n",
    "Allocating buffer space To actually use the `MPI_Bsend` we need also\n",
    "to allocate the space for the buffer, therefore we need to use the two\n",
    "functions\n",
    "\n",
    "``` c\n",
    "#define BUFFSIZE 10000\n",
    "int size; char *buff;\n",
    "// Buffer of 10000 bytes for MPI_Bsend\n",
    "MPI_Buffer_attach( malloc(BUFFSIZE), BUFFSIZE);\n",
    "// Buffer size reduced to zero \n",
    "MPI_Buffer_detach( &buff, &size);\n",
    "// Buffer of 10000 bytes available again \n",
    "MPI_Buffer_attach( buff, size); \n",
    "```\n",
    "\n",
    "``` {Warning}\n",
    "a pointer to the buffer is passed to\n",
    "`MPI_Buffer_attach` while the address of the pointer is passed to\n",
    "`MPI_Buffer_detach` and these are both `void *`.\n",
    "```\n",
    "\n",
    "## Nonblocking communications\n",
    "\n",
    "Nonblocking communications As we have seen the use of blocking\n",
    "communications ensures that\n",
    "\n",
    "-   the send and receive buffers used in the `MPI_Send` and\n",
    "    `MPI_Recv` arguments are safe to use or reuse after the function\n",
    "    call,\n",
    "\n",
    "-   but it also means that unless there is a simultaneously matching\n",
    "    send for each receive, the code will deadlock.\n",
    "\n",
    "There exists a version of the point-to-point communication that **returns\n",
    "immediately** from the function call before confirming that the\n",
    "send or the receive has completed, these are the nonblocking send and\n",
    "receive functions.\n",
    "\n",
    "-   To verify that the data has been copied out of the send buffer a\n",
    "    separate call is needed,\n",
    "\n",
    "-   To verify that the data has been received into the receive buffer a\n",
    "    separate call is needed,\n",
    "\n",
    "-   The sender should not modify any part of the send buffer after a\n",
    "    nonblocking send operation is called, until the send completes.\n",
    "\n",
    "-   The receiver should not access any part of the receive buffer after\n",
    "    a nonblocking receive operation is called, until the receive\n",
    "    completes.\n",
    "\n",
    "Nonblocking communications: `MPI_Isend` and `MPI_Irecv` The two\n",
    "nonblocking point-to-point communication call are then\n",
    "\n",
    "``` c\n",
    "int MPI_Isend(void *message, int count, \n",
    "   MPI_Datatype datatype, int dest, int tag,\n",
    "   MPI_Comm comm, MPI_Request *send_request);\n",
    "\n",
    "int MPI_Irecv(void *message, int count, \n",
    "   MPI_Datatype datatype, int source, int tag,\n",
    "   MPI_Comm comm, MPI_Request *recv_request);\n",
    "```\n",
    "\n",
    "-   The `MPI_Request` variables substitute the `MPI_Status` and\n",
    "    store information about the status of the pending communication\n",
    "    operation.\n",
    "\n",
    "-   The way of saying when this communications **must** be\n",
    "    completed is by using the when is called, the nonblocking request\n",
    "    originating from `MPI_Isend` or `MPI_Irecv` is provided as\n",
    "    an argument.\n",
    "\n",
    "Nonblocking communications: an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c520db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ccode/nonblockingsendrecv.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/nonblockingsendrecv.c\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    " int a, b, size, rank, tag = 0; \n",
    " MPI_Status status;\n",
    " MPI_Request send_request, recv_request;\n",
    " MPI_Init(&argc, &argv);\n",
    " MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    " MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "if (rank == 0) {\n",
    " a = 314159; \n",
    " MPI_Isend(&a, 1, MPI_INT, 1, tag, MPI_COMM_WORLD, &send_request);\n",
    " MPI_Irecv (&b, 1, MPI_INT, 1, tag, MPI_COMM_WORLD, &recv_request);\n",
    " MPI_Wait(&send_request, &status);\n",
    " MPI_Wait(&recv_request, &status);\n",
    " printf (\"Process %d received value %d\\n\", rank, b);\n",
    "} else {\n",
    " a = 667;\n",
    " MPI_Isend (&a, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &send_request);\n",
    " MPI_Irecv (&b, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &recv_request);\n",
    " MPI_Wait(&send_request, &status);\n",
    " MPI_Wait(&recv_request, &status);\n",
    " printf (\"Process %d received value %d\\n\", rank, b);\n",
    "}\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f03f2",
   "metadata": {},
   "source": [
    "A simple send/receive example We can compile our code by simply adding\n",
    "to our `Makefile`\n",
    "\n",
    "``` Makefile\n",
    "nonblockingsendrecv: nonblockingsendrecv.c\n",
    "  $(MPICC) $(CFLAGS) $(LDFLAGS) $? $(LDLIBS) -o $@\n",
    "```\n",
    "\n",
    "then, we type `make nonblockingsendrecv`, and we run our program with getting as\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3734773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc\t\t\t -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -g\t\t\t -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib nonblockingsendrecv.c  -o nonblockingsendrecv\n",
      "Process 0 received value 667\n",
      "Process 1 received value 314159\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make nonblockingsendrecv)\n",
    "!mpiexec -np 2 ./ccode/nonblockingsendrecv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f1309",
   "metadata": {},
   "source": [
    "Another useful instruction for the case of nonblocking\n",
    "communication is represented by\n",
    "\n",
    "``` c\n",
    "int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status);\n",
    "```\n",
    "\n",
    "A call to `MPI_TEST` returns `flag = true` if the operation\n",
    "identified by request is complete. In such a case, the status object is\n",
    "set to contain information on the completed operation.\n",
    "\n",
    "## Sendreceive\n",
    "\n",
    "Send-Receive The *send-receive* operations combine in one call\n",
    "the sending of a message to one destination and the receiving of another\n",
    "message, from another process.\n",
    "\n",
    "-   Source and destination are possibly the same,\n",
    "\n",
    "-   Send-receive operation is very useful for executing a shift\n",
    "    operation across a chain of processes,\n",
    "\n",
    "-   A message sent by a send-receive operation can be received by a\n",
    "    regular receive operation\n",
    "\n",
    "``` c\n",
    "int MPI_Sendrecv(const void *sendbuf, int sendcount, \n",
    "  MPI_Datatype sendtype, int dest, int sendtag, \n",
    "  void *recvbuf, int recvcount, MPI_Datatype recvtype, \n",
    "  int source, int recvtag, MPI_Comm comm,\n",
    "  MPI_Status *status);\n",
    "```\n",
    "\n",
    "Send-Receive-Replace A slight variant of the `MPI_Sendrecv`\n",
    "operation is represented by the `MPI_Sendrecv_replace` operation\n",
    "\n",
    "``` c\n",
    "int MPI_Sendrecv_replace(void* buf, int count, \n",
    "    MPI_Datatype datatype, int dest, int sendtag, \n",
    "    int source, int recvtag, \n",
    "    MPI_Comm comm, MPI_Status *status)\n",
    "```\n",
    "\n",
    "as the name suggests, the same buffer is used both for the send and for\n",
    "the receive, so that the message sent is replaced by the message\n",
    "received. Clearly, if you confront its arguments with the one of the\n",
    "`MPI_Sendrecv`, the arguments `void *recvbuf, int recvcount` are\n",
    "absent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb52edb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
