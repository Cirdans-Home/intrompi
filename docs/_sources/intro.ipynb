{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156e8406",
   "metadata": {},
   "source": [
    "# Message Passing Interface\n",
    "\n",
    "How do we realize practically this parallelism?\n",
    "\n",
    "Let us focus on what we have discussed until now:\n",
    "* We have ``machines'' with multiple processors and whose main memory is partitioned into fragmented components,\n",
    "* We have algorithms that can divide a problem of size $N$ among these processors so that they can run (almost) independently,\n",
    "* With a certain degree of approximation, we know how to compute what is the *best improvement* we can expect from a parallel program with $M$ processors on a problem of size $N$.\n",
    "\n",
    "What we need to discuss now is then: \"*How can we actually implement these algorithms on real machines?*\"\n",
    "* We need a way to define a parallel environment in which every processor is accounted for,\n",
    "* We need to have data formats that are aware of the fact that we have a *distributed* memory,\n",
    "* We need to exchange data between the various memory fragments.\n",
    "\n",
    ">\"MPI (Message Passing Interface) is a **specification for a standard library** for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists.\" -- W. Gropp, E. Lusk, N. Doss, A. Skjellum,'' -- A high-performance, portable implementation of the MPI message passing interface standard, Parallel Computing, 22 (6), 1996.\n",
    "\n",
    "* MPI implementations consist of a specific set of routines directly callable from C, C++, Fortran, Python;\n",
    "* MPI uses Language Independent Specifications for calls and language bindings;\n",
    "* The MPI interface provides an essential *virtual* topology, synchronization, and communication functionality inside a set of processes.\n",
    "* There exist many implementations of the MPI specification, e.g., MPICH, Open MPI, *etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8e9bd",
   "metadata": {},
   "source": [
    "## Our First MPI Program\n",
    "\n",
    "In all the course we are going to use the MPI inside Python programs.\n",
    "\n",
    "Let us start from the classical helloworld program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291d4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/helloworld.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/helloworld.c\n",
    "#include\"mpi.h\"\n",
    "#include<stdio.h>\n",
    "\n",
    "int main(int argc,char **argv){\n",
    " MPI_Init( &argc, &argv);\n",
    " printf(\"Hello, world!\\n\");\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd73b5",
   "metadata": {},
   "source": [
    "We can compile it by doing\n",
    "``` bash\n",
    "mpicc helloworld.c -o helloworld\n",
    "```\n",
    "- `mpicc` is a wrapper for a C compiler provided by the Open MPI implementation of MPI.\n",
    "- the option `-o` sets the name of the compiled (executable) file.\n",
    "\n",
    "Let us see what is happening behind the curtains\n",
    "- you can first try to discover what compiler are you using by executing \n",
    "```bash\n",
    "mpicc --version\n",
    "```\n",
    "that will give you something like\n",
    "```\n",
    "gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n",
    "Copyright (C) 2017 Free Software Foundation, Inc.\n",
    "```\n",
    "- or discover what are the library inclusion and linking options by asking for `mpicc --showme:compile` and `mpicc --showme:link`, respectively.\n",
    "- In general, looking at the output of the `man mpicc` command is always a good idea.\n",
    "\n",
    "> ``If you find yourself saying, \"But I don't want to use wrapper compilers!\", please humor us and try them. See if they work for you. Be sure to let us know if they do not work for you. '' - [https://www.open-mpi.org/faq/?category=mpi-apps](https://www.open-mpi.org/faq/?category=mpi-apps)\n",
    "\n",
    "```{note}\n",
    "A piece of advice: if your program is anything more realistic than a classroom exercise use `make`[^1], and save yourself from writing painfully long compiling commands, and dealing with complex dependencies more than once.\n",
    "```\n",
    "\n",
    "> \"Make gets its knowledge of how to build your program from a file called the makefile, which lists each of the non-source files and how to compute it from other files.\"\n",
    "\n",
    "A very simple `Makefile` for our first test would be\n",
    "``` make\n",
    "MPICC = mpicc #The wrapper for the compiler\n",
    "CFLAGS += -g  #Useful for debug symbols\n",
    "all: helloworld\n",
    "helloworld: helloworld.c\n",
    "  $(MPICC) $(CFLAGS) $(LDFLAGS) $? $(LDLIBS) -o $@\n",
    "clean:\n",
    "  rm -f helloworld\n",
    "```\n",
    "\n",
    "[^1]:[https://www.gnu.org/software/make/](https://www.gnu.org/software/make/)\n",
    "\n",
    "Let us run our first parallel program by doing:\n",
    "```bash\n",
    "mpirun [ -np X ] [ --hostfile <filename> ]  python helloworld.py\n",
    "```\n",
    "or by using its synonym\n",
    "```bash\n",
    "mpiexec [ -np X ] [ --hostfile <filename> ] python helloworld.py\n",
    "```\n",
    "* `mpiexec` will  run  `X` copies of `helloworld` in your current run-time environment, scheduling (by default) in a round-robin fashion by CPU slot.\n",
    "* if running under a supported resource manager, Open MPI's `mpirun` will usually automatically use the corresponding resource manager process starter, as opposed to, for example, rsh or ssh, which require the use of a hostfile, or will default  to  running all `X` copies on the localhost \n",
    "* as always, look at the manual, by doing `man mpirun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34ed81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cirdan/anaconda3/envs/parallel/bin/x86_64-conda_cos6-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/cirdan/anaconda3/envs/parallel/include -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib  helloworld.c   -o helloworld\n",
      "/home/cirdan/anaconda3/envs/parallel/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: /tmp/cc0XAkgc.o: in function `main':\n",
      "helloworld.c:(.text.startup.main+0x16): undefined reference to `MPI_Init'\n",
      "/home/cirdan/anaconda3/envs/parallel/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: helloworld.c:(.text.startup.main+0x29): undefined reference to `MPI_Finalize'\n",
      "collect2: error: ld returned 1 exit status\n",
      "<incorporato>: recipe for target 'helloworld' failed\n",
      "make: *** [helloworld] Error 1\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n",
      "[proxy:0:0@x580gd] HYDU_create_process (utils/launch/launch.c:74): execvp error on file ./ccode/helloworld (No such file or directory)\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make helloworld)\n",
    "!mpiexec -np 4 ./ccode/helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c7588",
   "metadata": {},
   "source": [
    "Every process executes the line that it is a **local** routine!\n",
    "\n",
    "> A procedure is **local** if completion\n",
    "of the procedure depends only on the local executing process.\n",
    "\n",
    ">A procedure is **non-local** if completion of the operation may require\n",
    "the execution of some MPI procedure on another process. Such an\n",
    "operation *may require communication* occurring with another user\n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d31737",
   "metadata": {},
   "source": [
    "## The MPI parallel environment\n",
    "\n",
    "The MPI parallel environment Let us modify our `helloworld` to\n",
    "investigate the MPI parallel environment. Specifically, we want to\n",
    "answer, from within the program, to the questions:\n",
    "\n",
    "1.  How many processes are there?\n",
    "\n",
    "2.  Who am I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66bcdadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/hamlet.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/hamlet.c\n",
    "#include \"mpi.h\"\n",
    "#include <stdio.h>\n",
    "int main( int argc, char **argv ){\n",
    " int rank, size;\n",
    " MPI_Init( &argc, &argv );\n",
    " MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n",
    " MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    " printf( \"Hello world! I'm process %d of %d\\n\",rank, size );\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c28adf",
   "metadata": {},
   "source": [
    "-   How many is answered by a call to `MPI_Comm_size` as an\n",
    "    `int` value,\n",
    "\n",
    "-   Who am I? Is answered by a call to `MPI_Comm_rank` as an\n",
    "    `int` value that is conventionally called `rank` and is a\n",
    "    number between `0` and `size-1`.\n",
    "\n",
    "The MPI parallel environment The last keyword we need to describe is the\n",
    "`MPI_COMM_WORLD`, this is the standard Communicator object.\n",
    "\n",
    "> **Communicator:** A Communicator object connects a group of processes in one\n",
    "MPI session. There can be more than one communicator in an MPI session,\n",
    "each of them gives each contained process an independent identifier and\n",
    "arranges its contained processes in an ordered topology.\n",
    "\n",
    "This provides\n",
    "\n",
    "-   a safe communication space, that guarantees that the code can\n",
    "    communicate as they need to, without conflicting with communication\n",
    "    extraneous to the present code, e.g., if other parallel libraries\n",
    "    are in use,\n",
    "\n",
    "-   a unified object for conveniently denoting communication context,\n",
    "    the group of communicating processes and to house abstract process\n",
    "    naming.\n",
    "\n",
    "The MPI parallel environment If we have saved our inquiring MPI program\n",
    "in the file `hamlet.c`, we can then modify our `Makefile`\n",
    "by modifying/adding the lines\n",
    "\n",
    "``` Makefile\n",
    "all: helloworld hamlet\n",
    "hamlet: hamlet.c\n",
    " $(MPICC) $(CFLAGS) $(LDFLAGS) $? $(LDLIBS) -o $@\n",
    "clean:\n",
    " rm -f helloworld hamlet\n",
    "```\n",
    "\n",
    "Then, we compile everything by doing `make hamlet` (or, simply,\n",
    "`make`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89904b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc\t\t\t -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -g\t\t\t -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib hamlet.c  -o hamlet\n",
      "Hello world! I'm process 0 of 6\n",
      "Hello world! I'm process 1 of 6\n",
      "Hello world! I'm process 2 of 6\n",
      "Hello world! I'm process 5 of 6\n",
      "Hello world! I'm process 3 of 6\n",
      "Hello world! I'm process 4 of 6\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make hamlet)\n",
    "!mpiexec -np 6 ./ccode/hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9850e",
   "metadata": {},
   "source": [
    "We can rewrite the same code in Python as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94045737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hamlet.py\n"
     ]
    }
   ],
   "source": [
    "%%file hamlet.py\n",
    "\"\"\"\n",
    "Hello (parallel) world!\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "size = comm.Get_size() \n",
    "\n",
    "print(\"Hello world! I'm process \",rank,\" of \",size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147cede",
   "metadata": {},
   "source": [
    "What have we done here:\n",
    "* The instruction \n",
    "```python\n",
    "from mpi4py import MPI\n",
    "```\n",
    "provides basic MPI definitions and types, if this was a `C` code, this would have been a *preprocessor* directive of the form `#include \"mpi.h\"`\n",
    "* start MPI by creating a communicator \n",
    "```python\n",
    "comm = MPI.COMM_WORLD\n",
    "```\n",
    "\n",
    "For the Python code\n",
    "\n",
    "* How many is answered by a call to `comm.Get_size()` as an `int` value,\n",
    "* Who am I? Is answered by a call to `comm.Get_rank()` as an `int` value that is conventionally called **rank** and is a number between `0` and `size-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeebc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I'm process  0  of  4\n",
      "Hello world! I'm process  1  of  4\n",
      "Hello world! I'm process  2  of  4\n",
      "Hello world! I'm process  3  of  4\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python hamlet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87ab4e",
   "metadata": {},
   "source": [
    "* Every processor answers the call,\n",
    "* But it answers it as soon as he has done doing the computation! There is **no synchronization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58425793",
   "metadata": {},
   "source": [
    "# Point-to-point communication\n",
    "\n",
    "Sending and Receiving Messages We have seen that each process within a\n",
    "*communicator* is identified by its *rank*, how can we [exchange\n",
    "data]{.alert} between two processes?\n",
    "\n",
    "We need to posses several information to have a meaningful message\n",
    "\n",
    "-   Who is sending the data?\n",
    "\n",
    "-   To whom the data is sent?\n",
    "\n",
    "-   What type of data are we sending?\n",
    "\n",
    "-   How does the receiver can identify it?\n",
    "\n",
    "## The blocking send and receive\n",
    "\n",
    "```c\n",
    "int MPI_Send(void *message, int count, \n",
    "    MPI_Datatype datatype, int dest, int tag, \n",
    "    MPI_Comm comm)\n",
    "```\n",
    "\n",
    "- `void *message`:   points to the message content itself, it can be a simple scalar or a\n",
    "    group of data,\n",
    "\n",
    "- `int count`:   specifies the number of data elements of which the message is\n",
    "    composed,\n",
    "\n",
    "- `MPI_Datatype datatype`:   indicates the [data type]{.alert} of the elements that make up the\n",
    "    message,\n",
    "\n",
    "- `int dest`:   the rank of the destination process,\n",
    "\n",
    "- `int tag`:   the user-defined tag field,\n",
    "\n",
    "- `MPI_Comm comm`:   the communicator in which the source and destination processes\n",
    "    reside and for which their respective ranks are defined.\n",
    "\n",
    "```c\n",
    "int MPI_Recv (void *message, int count, \n",
    "    MPI_Datatype datatype, int source, int tag,\n",
    "    MPI_Comm comm, MPI_Status *status)\n",
    "```\n",
    "\n",
    "- `void *message`:   points to the message content itself, it can be a simple scalar or a\n",
    "    group of data,\n",
    "\n",
    "- `int count`:   specifies the number of data elements of which the message is\n",
    "    composed,\n",
    "\n",
    "- `MPI_Datatype datatype`:   indicates the [data type]{.alert} of the elements that make up the\n",
    "    message,\n",
    "\n",
    "- `int dest`:   the rank of the source process,\n",
    "\n",
    "- `int tag`:   the user-defined tag field,\n",
    "\n",
    "- `MPI_Comm comm`:   the communicator in which the source and destination processes\n",
    "    reside,\n",
    "\n",
    "- `MPI_Status *status`:   is a structure that contains three fields named `MPI_SOURCE` ,\n",
    "    `MPI_TAG`, and `MPI_ERROR`.\n",
    "\n",
    "\n",
    "Basic MPI Data Types Of the inputs in the previous slides the only one\n",
    "that is specific to MPI is the `MPI_Datatype`, these corresponds to\n",
    "a C data type\n",
    "\n",
    "MPI Data Types | C Type\n",
    "---------------------------|-------------------------\n",
    "`MPI_CHAR`             |`signed char`\n",
    "`MPI_SHORT`            |`signed short int`\n",
    "`MPI_INT`              |`signed int`\n",
    "`MPI_LONG`             |`signed long int`\n",
    "`MPI_FLOAT`            |`float`\n",
    "`MPI_DOUBLE`           |`double`\n",
    "`MPI_LONG_DOUBLE`      |`long double`\n",
    "`MPI_UNSIGNED_CHAR`    |`unsigned char`\n",
    "`MPI_UNSIGNED_SHORT`   |`unsigned short int`\n",
    "`MPI_UNSIGNED`         |`unsigned int`\n",
    "`MPI_UNSIGNED_LONG`    |`unsigned long int`\n",
    "\n",
    "**Note:** we will see in the following how to\n",
    "`send`/`receive` user--defined data structures.\n",
    "\n",
    "Why \"blocking\" send and receive? For the `MPI_Send` to be\n",
    "**blocking** means that it does not return until the message data\n",
    "and envelope have been safely stored away so that the sender is free to\n",
    "modify the send buffer: it is a *non local* operation.\n",
    "\n",
    "**Note:** The message might be copied directly into the\n",
    "matching receive buffer (as in the first figure), or it might be copied\n",
    "into a temporary system buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1734b",
   "metadata": {},
   "source": [
    "## A simple send/receive example\n",
    "\n",
    "If we want to test these two instructions we can write the following simple C program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a4eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ccode/easysendrecv.c\n"
     ]
    }
   ],
   "source": [
    "%%file ccode/easysendrecv.c\n",
    "#include \"mpi.h\"\n",
    "#include <string.h>\n",
    "#include <stdio.h>\n",
    "int main( int argc, char **argv){\n",
    " char message[20];\n",
    " int myrank;\n",
    " MPI_Status status;\n",
    " MPI_Init( &argc, &argv );\n",
    " MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n",
    " if (myrank == 0){  /* code for process zero */\n",
    "  strcpy(message,\"Hello, there\");\n",
    "  MPI_Send(message, strlen(message)+1, MPI_CHAR, 1, 99, MPI_COMM_WORLD);\n",
    " }\n",
    " else if (myrank == 1){ /* code for process one */\n",
    "  MPI_Recv(message, 20, MPI_CHAR, 0, 99, MPI_COMM_WORLD, &status);\n",
    "  printf(\"received :%s:\\n\", message);\n",
    " }\n",
    " MPI_Finalize();\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d141d3",
   "metadata": {},
   "source": [
    "That could be recasted in Python by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58119c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting easysendrecv.py\n"
     ]
    }
   ],
   "source": [
    "%%file easysendrecv.py\n",
    "\"\"\"\n",
    "A simple send/receive example\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "size = comm.Get_size() \n",
    "\n",
    "if rank == 0:\n",
    "    data = \"Hello, there\"\n",
    "    comm.send(data, dest=1, tag=99)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=99)\n",
    "    print('received :',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665db0b",
   "metadata": {},
   "source": [
    "That we can run as the simpler program by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "215817c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received : Hello, there\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 2 python easysendrecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5192f",
   "metadata": {},
   "source": [
    "for the Python version, or the following for the C version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8d6ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc\t\t\t -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/cirdan/anaconda3/envs/parallel/include -g\t\t\t -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/cirdan/anaconda3/envs/parallel/lib -Wl,-rpath-link,/home/cirdan/anaconda3/envs/parallel/lib -L/home/cirdan/anaconda3/envs/parallel/lib easysendrecv.c  -o easysendrecv\n",
      "received :Hello, there:\n"
     ]
    }
   ],
   "source": [
    "!(cd ccode && make easysendrecv)\n",
    "!mpiexec -np 2 ./ccode/easysendrecv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd05723",
   "metadata": {},
   "source": [
    "So, what have we done? Process $0$ sends the content of the `char`\n",
    "array `message[20]`, whose size is `strlen(message)+1` size of\n",
    "`char` (`MPI_CHAR`) to processor `1` with tag `99` on\n",
    "the communicator `MPI_COMM_WORLD`. on the other side process $1$,\n",
    "receives into the buffer `message[20]` an array with size `20`\n",
    "size of `MPI_CHAR`, from process `0` with tag `99` on the\n",
    "same communicator `MPI_COMM_WORLD`.\n",
    "\n",
    "Observe that in the Python case we did not declare the size or the type of the object we were passing. The all-lowercase methods (of the `Comm` class), like `send()`, `recv()`, work by passing an object to be sent as a paramenter to the communication call, and the received object is simply the return value. These variants can communicate general Python objects.\n",
    "\n",
    "In MPI for Python, the `MPI.Comm.Send()`, `MPI.Comm.Recv()` and methods of communicator objects provide support for blocking point-to-point communications and can be used to communicate memory buffers, as we do in the C variant. Consider the following example sending a `numpy` array between two processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43c7e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting easysendrecv2.py\n"
     ]
    }
   ],
   "source": [
    "%%file easysendrecv2.py\n",
    "\"\"\"\n",
    "A (slightly less) simple send/receive example\n",
    "In which we :\n",
    "- pass MPI datatypes explicitly\n",
    "- use the MPI datatype discovery\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = numpy.arange(1000, dtype='i')\n",
    "    comm.Send([data, MPI.INT], dest=1, tag=77)\n",
    "elif rank == 1:\n",
    "    data = numpy.empty(1000, dtype='i')\n",
    "    comm.Recv([data, MPI.INT], source=0, tag=77)\n",
    "\n",
    "if rank == 0:\n",
    "    data = numpy.arange(100, dtype=numpy.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "elif rank == 1:\n",
    "    data = numpy.empty(100, dtype=numpy.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e5d7e",
   "metadata": {},
   "source": [
    "In general, buffer arguments to these calls must be explicitly specified by using a 2/3-list/tuple like `[data, MPI.DOUBLE]`, or `[data, count, MPI.DOUBLE]` (the former one uses the byte-size of data and the extent of the MPI datatype to define count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164ad625",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python easysendrecv2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67be5b5",
   "metadata": {},
   "source": [
    "The perform the datatype discovery, Python uses the `pickle` module. This module implements binary protocols for serializing and de-serializing a Python object structure. \"Pickling\" is the process converting a Python object hierarchy into a byte stream, and \"unpickling\" is the inverse operation, converting a byte stream (from a binary file or bytes-like object) into an object hierarchy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ba349",
   "metadata": {},
   "source": [
    "### A simple send/receive example : programmer smash!\n",
    "\n",
    "It is a good exercise\n",
    "to try and mess things up, so let us see some damaging suggestions (test them with the previous C code):\n",
    "\n",
    "-   What happens if we have a mismatch in the tags?\n",
    "\n",
    "-   **A:** The process stays there hanging waiting for a message with a tag\n",
    "    that will never come...\n",
    "\n",
    "-   What happens if we have a mismatch in the ranks of the sending and\n",
    "    receiving processes?\n",
    "\n",
    "-    **A:** The process stays there hanging trying to match messages that will\n",
    "    never come...\n",
    "\n",
    "-   What happens if we use the wrong message size?\n",
    "\n",
    "-    **A:** If the size of the arriving message is longer than the expected we\n",
    "    get an error of `MPI_ERR_TRUNCATE: message truncated`, note\n",
    "    that there are combinations of wrong sizes for which things still\n",
    "    works\n",
    "\n",
    "-   What happens if we have a mismatch in the type?\n",
    "\n",
    "-   **A:** There are combinations of instances in which things seems to work,\n",
    "    **but** the code is erroneous, and the behavior is not\n",
    "    deterministic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
